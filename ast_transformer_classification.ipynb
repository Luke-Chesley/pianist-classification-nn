{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 10:38:49.436433: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-12 10:38:49.562047: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-12 10:38:50.124010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%run local_functions.py\n",
    "from local_functions import *\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "import os\n",
    "import ffmpeg\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import EvalPrediction\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 2500)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into 5 sec clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob.glob(\"audio_files/**/*.mp3\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_audio(input_file, output_dir, clip_duration=5):\n",
    "    input_file = os.path.abspath(input_file)\n",
    "    output_dir = os.path.abspath(output_dir)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get the duration of the input audio file\n",
    "    probe = ffmpeg.probe(\n",
    "        input_file, v=\"error\", select_streams=\"a:0\", show_entries=\"format=duration\"\n",
    "    )\n",
    "    duration = float(probe[\"format\"][\"duration\"])\n",
    "\n",
    "    # Calculate the number of clips needed\n",
    "    num_clips = ceil(duration / clip_duration)\n",
    "\n",
    "    # Get the base filename without extension\n",
    "    base_filename = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "    # Split the audio into clips\n",
    "    for i in range(num_clips):\n",
    "        start_time = i * clip_duration\n",
    "        end_time = min((i + 1) * clip_duration, duration)\n",
    "\n",
    "        # Generate a unique filename for each clip\n",
    "        clip_filename = (\n",
    "            f\"{base_filename}_clip_{i + 1}.mp3\"  # Change the extension if needed\n",
    "        )\n",
    "        output_file = os.path.join(output_dir, clip_filename)\n",
    "\n",
    "        ffmpeg.input(input_file, ss=start_time, to=end_time).output(output_file).run(\n",
    "            overwrite_output=True\n",
    "        )\n",
    "\n",
    "    print(f\"{input_file} split into {num_clips} clips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in audio_files:\n",
    "#    artist = file.split(\"/\")[1].split()[0]\n",
    "\n",
    "#    if __name__ == \"__main__\":\n",
    "#        input_audio = file  # Replace with your input audio file\n",
    "#        output_directory = os.path.join(\n",
    "#            \"split_train_files\", artist\n",
    "#        )  # Construct the output directory path\n",
    "\n",
    "#       split_audio(input_audio, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_random_files_from_subdirs(\"split_train_files\", num_files=100)\n",
    "labels = [file.split()[0].split(\"/\")[-1] for file in files]\n",
    "df = pd.DataFrame({\"file\": files, \"label\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"features\"] = df[\"file\"].apply(file_to_librosa_features, args=(160000,))\n",
    "df.drop(\"file\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.get_dummies(df[\"label\"], columns=[\"label\"], prefix=\"\", prefix_sep=\"\")\n",
    "encoded_df = encoded_df.astype(bool)\n",
    "df = pd.concat([df[\"features\"], encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert the split DataFrames into Datasets\n",
    "train = Dataset.from_pandas(train_df, split=\"train\")\n",
    "valid = Dataset.from_pandas(valid_df, split=\"validation\")\n",
    "test = Dataset.from_pandas(test_df, split=\"test\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train, \"validation\": valid, \"test\": test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    label\n",
    "    for label in dataset[\"train\"].features.keys()\n",
    "    if label not in [\"features\", \"__index_level_0__\"]\n",
    "]\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    features = examples[\"features\"]\n",
    "    # encode them\n",
    "    encoding = extractor(features, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(features), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97285fee97284264af123b938f79575e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/840 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/luke/projects/jupyterlab/Notebooks/librosa/ast_transformer_classification.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/luke/projects/jupyterlab/Notebooks/librosa/ast_transformer_classification.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m encoded_dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoded_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9753, -1.2468, -0.8700,  ..., -1.2776, -1.2776, -1.2776],\n",
       "        [-1.1615, -1.0551, -0.6783,  ..., -0.7615, -1.2640, -1.2776],\n",
       "        [-0.5504, -0.9755, -0.5987,  ..., -0.4456, -1.0860, -1.2776],\n",
       "        ...,\n",
       "        [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],\n",
       "        [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670],\n",
       "        [ 0.4670,  0.4670,  0.4670,  ...,  0.4670,  0.4670,  0.4670]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset[\"train\"][\"input_values\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([12, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"ast-finetuned-ks\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-8,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {\"f1\": f1_micro_average, \"roc_auc\": roc_auc, \"accuracy\": accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics_2(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(predictions=preds, labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=extractor,\n",
    "    compute_metrics=compute_metrics_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40aa08b9f49b4f39921336926f47ea5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7336, 'learning_rate': 5.660377358490566e-09, 'epoch': 0.1}\n",
      "{'loss': 0.7342, 'learning_rate': 1.1320754716981132e-08, 'epoch': 0.19}\n",
      "{'loss': 0.7403, 'learning_rate': 1.6981132075471695e-08, 'epoch': 0.29}\n",
      "{'loss': 0.7204, 'learning_rate': 2.2641509433962263e-08, 'epoch': 0.38}\n",
      "{'loss': 0.724, 'learning_rate': 2.830188679245283e-08, 'epoch': 0.48}\n",
      "{'loss': 0.7154, 'learning_rate': 2.955508474576271e-08, 'epoch': 0.57}\n",
      "{'loss': 0.7285, 'learning_rate': 2.8919491525423726e-08, 'epoch': 0.67}\n",
      "{'loss': 0.7285, 'learning_rate': 2.8283898305084745e-08, 'epoch': 0.76}\n",
      "{'loss': 0.7118, 'learning_rate': 2.764830508474576e-08, 'epoch': 0.86}\n",
      "{'loss': 0.7167, 'learning_rate': 2.701271186440678e-08, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fc48f478b3459e811f5a22d89fbe25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7002573609352112, 'eval_f1': 0.1608040201005025, 'eval_roc_auc': 0.5348484848484848, 'eval_accuracy': 0.0, 'eval_runtime': 8.0687, 'eval_samples_per_second': 22.308, 'eval_steps_per_second': 11.154, 'epoch': 1.0}\n",
      "{'loss': 0.6997, 'learning_rate': 2.6377118644067792e-08, 'epoch': 1.05}\n",
      "{'loss': 0.7068, 'learning_rate': 2.574152542372881e-08, 'epoch': 1.14}\n",
      "{'loss': 0.7109, 'learning_rate': 2.5105932203389827e-08, 'epoch': 1.24}\n",
      "{'loss': 0.6887, 'learning_rate': 2.4470338983050847e-08, 'epoch': 1.33}\n",
      "{'loss': 0.682, 'learning_rate': 2.3834745762711862e-08, 'epoch': 1.43}\n",
      "{'loss': 0.6779, 'learning_rate': 2.319915254237288e-08, 'epoch': 1.52}\n",
      "{'loss': 0.7022, 'learning_rate': 2.2563559322033897e-08, 'epoch': 1.62}\n",
      "{'loss': 0.6851, 'learning_rate': 2.1927966101694913e-08, 'epoch': 1.71}\n",
      "{'loss': 0.6814, 'learning_rate': 2.129237288135593e-08, 'epoch': 1.81}\n",
      "{'loss': 0.6788, 'learning_rate': 2.0656779661016948e-08, 'epoch': 1.9}\n",
      "{'loss': 0.6697, 'learning_rate': 2.0021186440677964e-08, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc92cb210dc44da8ad527eec8ae6801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6683839559555054, 'eval_f1': 0.16431095406360427, 'eval_roc_auc': 0.5414141414141413, 'eval_accuracy': 0.0, 'eval_runtime': 7.9207, 'eval_samples_per_second': 22.725, 'eval_steps_per_second': 11.363, 'epoch': 2.0}\n",
      "{'loss': 0.6735, 'learning_rate': 1.9385593220338983e-08, 'epoch': 2.1}\n",
      "{'loss': 0.6552, 'learning_rate': 1.875e-08, 'epoch': 2.19}\n",
      "{'loss': 0.668, 'learning_rate': 1.8114406779661015e-08, 'epoch': 2.29}\n",
      "{'loss': 0.665, 'learning_rate': 1.747881355932203e-08, 'epoch': 2.38}\n",
      "{'loss': 0.6903, 'learning_rate': 1.6843220338983047e-08, 'epoch': 2.48}\n",
      "{'loss': 0.6568, 'learning_rate': 1.6207627118644066e-08, 'epoch': 2.57}\n",
      "{'loss': 0.6533, 'learning_rate': 1.5572033898305082e-08, 'epoch': 2.67}\n",
      "{'loss': 0.6652, 'learning_rate': 1.49364406779661e-08, 'epoch': 2.76}\n",
      "{'loss': 0.652, 'learning_rate': 1.4300847457627117e-08, 'epoch': 2.86}\n",
      "{'loss': 0.6478, 'learning_rate': 1.3665254237288134e-08, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367f3efd035745db87b30c703d7113b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6462792158126831, 'eval_f1': 0.1541318477251625, 'eval_roc_auc': 0.5250000000000001, 'eval_accuracy': 0.0, 'eval_runtime': 7.9183, 'eval_samples_per_second': 22.732, 'eval_steps_per_second': 11.366, 'epoch': 3.0}\n",
      "{'loss': 0.6421, 'learning_rate': 1.3029661016949152e-08, 'epoch': 3.05}\n",
      "{'loss': 0.6236, 'learning_rate': 1.239406779661017e-08, 'epoch': 3.14}\n",
      "{'loss': 0.6585, 'learning_rate': 1.1758474576271185e-08, 'epoch': 3.24}\n",
      "{'loss': 0.644, 'learning_rate': 1.1122881355932203e-08, 'epoch': 3.33}\n",
      "{'loss': 0.647, 'learning_rate': 1.048728813559322e-08, 'epoch': 3.43}\n",
      "{'loss': 0.6455, 'learning_rate': 9.851694915254236e-09, 'epoch': 3.52}\n",
      "{'loss': 0.6479, 'learning_rate': 9.216101694915254e-09, 'epoch': 3.62}\n",
      "{'loss': 0.6418, 'learning_rate': 8.580508474576271e-09, 'epoch': 3.71}\n",
      "{'loss': 0.6501, 'learning_rate': 7.944915254237289e-09, 'epoch': 3.81}\n",
      "{'loss': 0.6415, 'learning_rate': 7.3093220338983044e-09, 'epoch': 3.9}\n",
      "{'loss': 0.6432, 'learning_rate': 6.673728813559321e-09, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936c5b99225c4baeb5f2e1761557216f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6333296895027161, 'eval_f1': 0.14942528735632182, 'eval_roc_auc': 0.5181818181818182, 'eval_accuracy': 0.0, 'eval_runtime': 7.927, 'eval_samples_per_second': 22.707, 'eval_steps_per_second': 11.354, 'epoch': 4.0}\n",
      "{'loss': 0.6285, 'learning_rate': 6.038135593220338e-09, 'epoch': 4.1}\n",
      "{'loss': 0.6335, 'learning_rate': 5.402542372881355e-09, 'epoch': 4.19}\n",
      "{'loss': 0.6431, 'learning_rate': 4.766949152542372e-09, 'epoch': 4.29}\n",
      "{'loss': 0.6405, 'learning_rate': 4.1313559322033895e-09, 'epoch': 4.38}\n",
      "{'loss': 0.6385, 'learning_rate': 3.495762711864406e-09, 'epoch': 4.48}\n",
      "{'loss': 0.6227, 'learning_rate': 2.8601694915254233e-09, 'epoch': 4.57}\n",
      "{'loss': 0.6395, 'learning_rate': 2.2245762711864404e-09, 'epoch': 4.67}\n",
      "{'loss': 0.6371, 'learning_rate': 1.5889830508474575e-09, 'epoch': 4.76}\n",
      "{'loss': 0.6403, 'learning_rate': 9.533898305084744e-10, 'epoch': 4.86}\n",
      "{'loss': 0.6307, 'learning_rate': 3.177966101694915e-10, 'epoch': 4.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7580bfd07e2e40af8ffb4f20a199fe3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6290994882583618, 'eval_f1': 0.1475728155339806, 'eval_roc_auc': 0.5156565656565657, 'eval_accuracy': 0.0, 'eval_runtime': 7.917, 'eval_samples_per_second': 22.736, 'eval_steps_per_second': 11.368, 'epoch': 5.0}\n",
      "{'train_runtime': 578.6124, 'train_samples_per_second': 7.259, 'train_steps_per_second': 0.907, 'train_loss': 0.6708842518216088, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=525, training_loss=0.6708842518216088, metrics={'train_runtime': 578.6124, 'train_samples_per_second': 7.259, 'train_steps_per_second': 0.907, 'train_loss': 0.6708842518216088, 'epoch': 5.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fb88bee4494e65aef5c8d84b69f668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7002573609352112,\n",
       " 'eval_f1': 0.1608040201005025,\n",
       " 'eval_roc_auc': 0.5348484848484848,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 7.8671,\n",
       " 'eval_samples_per_second': 22.88,\n",
       " 'eval_steps_per_second': 11.44,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"ast-finetuned-ks/checkpoint-525\",\n",
    "    num_labels=len(id2label),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, tokenizer=extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_random_files_from_subdirs(\"split_train_files\", num_files=100)\n",
    "labels = [file.split()[0].split(\"/\")[-1] for file in files]\n",
    "df_test = pd.DataFrame({\"file\": files, \"label\": labels})\n",
    "df_test[\"features\"] = df_test[\"file\"].apply(file_to_librosa_features)\n",
    "df_test.drop(\"file\", axis=1, inplace=True)\n",
    "\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[-4.8818765e-07, -9.2718983e-07, -6.4910273e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schiff</td>\n",
       "      <td>[-0.008452152, -0.017701644, -0.018430043, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Horowitz</td>\n",
       "      <td>[-7.096678e-07, -1.4826655e-06, -1.5776604e-06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Richter</td>\n",
       "      <td>[-1.6298145e-09, 1.3969839e-09, -1.7462298e-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crochet</td>\n",
       "      <td>[-2.7939677e-09, -1.3969839e-09, -1.3969839e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>Pogorelich</td>\n",
       "      <td>[-5.005859e-09, -6.4028427e-09, -2.3283064e-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>Pogorelich</td>\n",
       "      <td>[-9.313226e-10, -1.2805685e-09, -5.820766e-10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[1.7369166e-07, 3.091991e-07, 2.5704503e-07, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>Schiff</td>\n",
       "      <td>[-1.6778418e-10, -1.03521386e-10, -3.929264e-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[1.4994293e-07, 2.9057264e-07, 2.477318e-07, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                           features\n",
       "0          Gould  [-4.8818765e-07, -9.2718983e-07, -6.4910273e-0...\n",
       "1         Schiff  [-0.008452152, -0.017701644, -0.018430043, -0....\n",
       "2       Horowitz  [-7.096678e-07, -1.4826655e-06, -1.5776604e-06...\n",
       "3        Richter  [-1.6298145e-09, 1.3969839e-09, -1.7462298e-09...\n",
       "4        Crochet  [-2.7939677e-09, -1.3969839e-09, -1.3969839e-0...\n",
       "...          ...                                                ...\n",
       "1195  Pogorelich  [-5.005859e-09, -6.4028427e-09, -2.3283064e-09...\n",
       "1196  Pogorelich  [-9.313226e-10, -1.2805685e-09, -5.820766e-10,...\n",
       "1197       Gould  [1.7369166e-07, 3.091991e-07, 2.5704503e-07, 2...\n",
       "1198      Schiff  [-1.6778418e-10, -1.03521386e-10, -3.929264e-1...\n",
       "1199       Gould  [1.4994293e-07, 2.9057264e-07, 2.477318e-07, 2...\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(initial_features, trainer, extractor, id2label, CONFIDENCE_THRESHOLD=0.5):\n",
    "    encoding = extractor(initial_features, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(trainer.model.device) for k, v in encoding.items()}\n",
    "\n",
    "    outputs = trainer.model(**encoding)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # apply sigmoid + threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    predictions = np.zeros(probs.shape)\n",
    "\n",
    "    predictions[np.where(probs >= CONFIDENCE_THRESHOLD)] = 1\n",
    "    # turn predicted id's into actual label names\n",
    "    predicted_labels = [\n",
    "        id2label[idx] for idx, label in enumerate(predictions) if label == 1.0\n",
    "    ]\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_TO_EVALUATE = 500\n",
    "CONFIDENCE_THRESHOLD = 0.6\n",
    "\n",
    "df_test[\"predicted_label\"] = df_test[\"features\"][0:ROWS_TO_EVALUATE].apply(\n",
    "    inference, args=(trainer, extractor, id2label, CONFIDENCE_THRESHOLD)\n",
    ")\n",
    "\n",
    "df_test[\"correct\"] = df_test.apply(\n",
    "    lambda row: int(str(row[\"label\"]) in str(row[\"predicted_label\"])), axis=1\n",
    ")\n",
    "\n",
    "df_test[\"top_n_preds\"] = df_test[\"features\"][0:ROWS_TO_EVALUATE].apply(\n",
    "    n_most_likely_classes, args=(trainer, extractor, id2label, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>correct</th>\n",
       "      <th>top_n_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[-4.8818765e-07, -9.2718983e-07, -6.4910273e-0...</td>\n",
       "      <td>[Horowitz, Richter, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Tureck': 0.654, 'Richter': 0.639, 'Horowitz'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schiff</td>\n",
       "      <td>[-0.008452152, -0.017701644, -0.018430043, -0....</td>\n",
       "      <td>[Horowitz, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.661, 'Tureck': 0.611, 'Schiff':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Horowitz</td>\n",
       "      <td>[-7.096678e-07, -1.4826655e-06, -1.5776604e-06...</td>\n",
       "      <td>[Horowitz, Ishizaka]</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Horowitz': 0.699, 'Ishizaka': 0.657, 'Richte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Richter</td>\n",
       "      <td>[-1.6298145e-09, 1.3969839e-09, -1.7462298e-09...</td>\n",
       "      <td>[Horowitz]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.753, 'Tureck': 0.57, 'Richter':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crochet</td>\n",
       "      <td>[-2.7939677e-09, -1.3969839e-09, -1.3969839e-0...</td>\n",
       "      <td>[Richter, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Tureck': 0.638, 'Richter': 0.621, 'Horowitz'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Richter</td>\n",
       "      <td>[-8.940697e-08, 2.6077032e-08, -4.7683716e-07,...</td>\n",
       "      <td>[Horowitz]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.674, 'Tureck': 0.6, 'Ishizaka':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[-1.6746654e-10, -7.866863e-11, -1.3521877e-10...</td>\n",
       "      <td>[Horowitz, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Tureck': 0.666, 'Horowitz': 0.652, 'Ishizaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Tureck</td>\n",
       "      <td>[-3.434252e-08, -1.14087015e-08, -3.934838e-08...</td>\n",
       "      <td>[Richter]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Richter': 0.615, 'Moravec': 0.581, 'Ishizaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[-2.0256266e-07, -3.259629e-07, -2.9057264e-07...</td>\n",
       "      <td>[Horowitz, Ishizaka, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.644, 'Tureck': 0.64, 'Ishizaka'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Pogorelich</td>\n",
       "      <td>[9.313226e-10, 1.8626451e-09, 9.313226e-10, 9....</td>\n",
       "      <td>[Horowitz, Richter]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.674, 'Richter': 0.646, 'Schiff'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                           features  \\\n",
       "0         Gould  [-4.8818765e-07, -9.2718983e-07, -6.4910273e-0...   \n",
       "1        Schiff  [-0.008452152, -0.017701644, -0.018430043, -0....   \n",
       "2      Horowitz  [-7.096678e-07, -1.4826655e-06, -1.5776604e-06...   \n",
       "3       Richter  [-1.6298145e-09, 1.3969839e-09, -1.7462298e-09...   \n",
       "4       Crochet  [-2.7939677e-09, -1.3969839e-09, -1.3969839e-0...   \n",
       "..          ...                                                ...   \n",
       "495     Richter  [-8.940697e-08, 2.6077032e-08, -4.7683716e-07,...   \n",
       "496       Gould  [-1.6746654e-10, -7.866863e-11, -1.3521877e-10...   \n",
       "497      Tureck  [-3.434252e-08, -1.14087015e-08, -3.934838e-08...   \n",
       "498       Gould  [-2.0256266e-07, -3.259629e-07, -2.9057264e-07...   \n",
       "499  Pogorelich  [9.313226e-10, 1.8626451e-09, 9.313226e-10, 9....   \n",
       "\n",
       "                  predicted_label  correct  \\\n",
       "0     [Horowitz, Richter, Tureck]        0   \n",
       "1              [Horowitz, Tureck]        0   \n",
       "2            [Horowitz, Ishizaka]        1   \n",
       "3                      [Horowitz]        0   \n",
       "4               [Richter, Tureck]        0   \n",
       "..                            ...      ...   \n",
       "495                    [Horowitz]        0   \n",
       "496            [Horowitz, Tureck]        0   \n",
       "497                     [Richter]        0   \n",
       "498  [Horowitz, Ishizaka, Tureck]        0   \n",
       "499           [Horowitz, Richter]        0   \n",
       "\n",
       "                                           top_n_preds  \n",
       "0    {'Tureck': 0.654, 'Richter': 0.639, 'Horowitz'...  \n",
       "1    {'Horowitz': 0.661, 'Tureck': 0.611, 'Schiff':...  \n",
       "2    {'Horowitz': 0.699, 'Ishizaka': 0.657, 'Richte...  \n",
       "3    {'Horowitz': 0.753, 'Tureck': 0.57, 'Richter':...  \n",
       "4    {'Tureck': 0.638, 'Richter': 0.621, 'Horowitz'...  \n",
       "..                                                 ...  \n",
       "495  {'Horowitz': 0.674, 'Tureck': 0.6, 'Ishizaka':...  \n",
       "496  {'Tureck': 0.666, 'Horowitz': 0.652, 'Ishizaka...  \n",
       "497  {'Richter': 0.615, 'Moravec': 0.581, 'Ishizaka...  \n",
       "498  {'Horowitz': 0.644, 'Tureck': 0.64, 'Ishizaka'...  \n",
       "499  {'Horowitz': 0.674, 'Richter': 0.646, 'Schiff'...  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[0:ROWS_TO_EVALUATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[0:ROWS_TO_EVALUATE].correct.sum() / ROWS_TO_EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
