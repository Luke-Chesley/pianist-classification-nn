{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 14:09:30.928585: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-12 14:09:31.079090: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-12 14:09:31.680068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%run local_functions.py\n",
    "from local_functions import *\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "import os\n",
    "import ffmpeg\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import EvalPrediction\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 2500)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into 5 sec clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob.glob(\"audio_files/**/*.mp3\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_audio(input_file, output_dir, clip_duration=5):\n",
    "    input_file = os.path.abspath(input_file)\n",
    "    output_dir = os.path.abspath(output_dir)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get the duration of the input audio file\n",
    "    probe = ffmpeg.probe(\n",
    "        input_file, v=\"error\", select_streams=\"a:0\", show_entries=\"format=duration\"\n",
    "    )\n",
    "    duration = float(probe[\"format\"][\"duration\"])\n",
    "\n",
    "    # Calculate the number of clips needed\n",
    "    num_clips = ceil(duration / clip_duration)\n",
    "\n",
    "    # Get the base filename without extension\n",
    "    base_filename = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "    # Split the audio into clips\n",
    "    for i in range(num_clips):\n",
    "        start_time = i * clip_duration\n",
    "        end_time = min((i + 1) * clip_duration, duration)\n",
    "\n",
    "        # Generate a unique filename for each clip\n",
    "        clip_filename = (\n",
    "            f\"{base_filename}_clip_{i + 1}.mp3\"  # Change the extension if needed\n",
    "        )\n",
    "        output_file = os.path.join(output_dir, clip_filename)\n",
    "\n",
    "        ffmpeg.input(input_file, ss=start_time, to=end_time).output(output_file).run(\n",
    "            overwrite_output=True\n",
    "        )\n",
    "\n",
    "    print(f\"{input_file} split into {num_clips} clips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in audio_files:\n",
    "#    artist = file.split(\"/\")[1].split()[0]\n",
    "\n",
    "#    if __name__ == \"__main__\":\n",
    "#        input_audio = file  # Replace with your input audio file\n",
    "#        output_directory = os.path.join(\n",
    "#            \"split_train_files\", artist\n",
    "#        )  # Construct the output directory path\n",
    "\n",
    "#       split_audio(input_audio, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_random_files_from_subdirs(\"split_train_files\", num_files=10)\n",
    "labels = [file.split()[0].split(\"/\")[-1] for file in files]\n",
    "df = pd.DataFrame({\"file\": files, \"label\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"features\"] = df[\"file\"].apply(file_to_librosa_features, args=(160000,))\n",
    "df.drop(\"file\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(df[\"label\"], columns=[\"label\"], prefix=\"\", prefix_sep=\"\")\n",
    "dummy_df = dummy_df.astype(bool)\n",
    "df = pd.concat([df[\"features\"], dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert the split DataFrames into Datasets\n",
    "train = Dataset.from_pandas(train_df, split=\"train\")\n",
    "valid = Dataset.from_pandas(valid_df, split=\"validation\")\n",
    "test = Dataset.from_pandas(test_df, split=\"test\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train, \"validation\": valid, \"test\": test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['features', 'Crochet', 'Gould', 'Horowitz', 'Ishizaka', 'Moravec', 'Nikolayeva', 'Pogorelich', 'Richter', 'Rubinstein', 'Schiff', 'Tharaud', 'Tureck', '__index_level_0__'],\n",
       "        num_rows: 84\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['features', 'Crochet', 'Gould', 'Horowitz', 'Ishizaka', 'Moravec', 'Nikolayeva', 'Pogorelich', 'Richter', 'Rubinstein', 'Schiff', 'Tharaud', 'Tureck', '__index_level_0__'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['features', 'Crochet', 'Gould', 'Horowitz', 'Ishizaka', 'Moravec', 'Nikolayeva', 'Pogorelich', 'Richter', 'Rubinstein', 'Schiff', 'Tharaud', 'Tureck', '__index_level_0__'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    label\n",
    "    for label in dataset[\"train\"].features.keys()\n",
    "    if label not in [\"features\", \"__index_level_0__\"]\n",
    "]\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\", max_length=800000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"ASTFeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"max_length\": 800000,\n",
       "  \"mean\": -4.2677393,\n",
       "  \"num_mel_bins\": 128,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000,\n",
       "  \"std\": 4.5689974\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 800000, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor(\n",
    "    dataset[\"train\"][0][\"features\"], sampling_rate=16000, return_tensors=\"pt\"\n",
    ").input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    features = examples[\"features\"]\n",
    "    # encode them\n",
    "    encoding = extractor(features, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    # add labels\n",
    "    # labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    # labels_matrix = np.zeros((len(features), len(labels)))\n",
    "    # fill numpy array\n",
    "    # for idx, label in enumerate(labels):\n",
    "    #    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    # encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "did = preprocess_data(dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877765040f47409199bdc0198d40109d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    preprocess_data, batched=False, remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values'],\n",
       "        num_rows: 840\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_values'],\n",
       "        num_rows: 180\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values'],\n",
       "        num_rows: 180\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 128])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset[\"train\"][\"input_values\"][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([12, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"ast-finetuned-ks\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-8,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {\"f1\": f1_micro_average, \"roc_auc\": roc_auc, \"accuracy\": accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics_2(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(predictions=preds, labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=extractor,\n",
    "    compute_metrics=compute_metrics_2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a6df328bab49caaa02b370cf5d9c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 1, 1024, 1, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/luke/projects/jupyterlab/Notebooks/pianist-classification-nn/ast_transformer_classification.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/luke/projects/jupyterlab/Notebooks/pianist-classification-nn/ast_transformer_classification.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2653\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2654\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2657\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2678\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2679\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2680\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:584\u001b[0m, in \u001b[0;36mASTForAudioClassification.forward\u001b[0;34m(self, input_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[39m    Labels for computing the audio classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    582\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 584\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_spectrogram_transformer(\n\u001b[1;32m    585\u001b[0m     input_values,\n\u001b[1;32m    586\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    587\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    588\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    590\u001b[0m )\n\u001b[1;32m    592\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    593\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:501\u001b[0m, in \u001b[0;36mASTModel.forward\u001b[0;34m(self, input_values, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    499\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 501\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_values)\n\u001b[1;32m    503\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    504\u001b[0m     embedding_output,\n\u001b[1;32m    505\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    509\u001b[0m )\n\u001b[1;32m    510\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:82\u001b[0m, in \u001b[0;36mASTEmbeddings.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_values: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     81\u001b[0m     batch_size \u001b[39m=\u001b[39m input_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 82\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embeddings(input_values)\n\u001b[1;32m     84\u001b[0m     cls_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token\u001b[39m.\u001b[39mexpand(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m     distillation_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistillation_token\u001b[39m.\u001b[39mexpand(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py:113\u001b[0m, in \u001b[0;36mASTPatchEmbeddings.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    111\u001b[0m input_values \u001b[39m=\u001b[39m input_values\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    112\u001b[0m input_values \u001b[39m=\u001b[39m input_values\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprojection(input_values)\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 1, 1024, 1, 128]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fb88bee4494e65aef5c8d84b69f668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7002573609352112,\n",
       " 'eval_f1': 0.1608040201005025,\n",
       " 'eval_roc_auc': 0.5348484848484848,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 7.8671,\n",
       " 'eval_samples_per_second': 22.88,\n",
       " 'eval_steps_per_second': 11.44,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"ast-finetuned-ks/checkpoint-525\",\n",
    "    num_labels=len(id2label),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, tokenizer=extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_random_files_from_subdirs(\"split_train_files\", num_files=100)\n",
    "labels = [file.split()[0].split(\"/\")[-1] for file in files]\n",
    "df_test = pd.DataFrame({\"file\": files, \"label\": labels})\n",
    "df_test[\"features\"] = df_test[\"file\"].apply(file_to_librosa_features)\n",
    "df_test.drop(\"file\", axis=1, inplace=True)\n",
    "\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[-4.8818765e-07, -9.2718983e-07, -6.4910273e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schiff</td>\n",
       "      <td>[-0.008452152, -0.017701644, -0.018430043, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Horowitz</td>\n",
       "      <td>[-7.096678e-07, -1.4826655e-06, -1.5776604e-06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Richter</td>\n",
       "      <td>[-1.6298145e-09, 1.3969839e-09, -1.7462298e-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crochet</td>\n",
       "      <td>[-2.7939677e-09, -1.3969839e-09, -1.3969839e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>Pogorelich</td>\n",
       "      <td>[-5.005859e-09, -6.4028427e-09, -2.3283064e-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>Pogorelich</td>\n",
       "      <td>[-9.313226e-10, -1.2805685e-09, -5.820766e-10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[1.7369166e-07, 3.091991e-07, 2.5704503e-07, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>Schiff</td>\n",
       "      <td>[-1.6778418e-10, -1.03521386e-10, -3.929264e-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[1.4994293e-07, 2.9057264e-07, 2.477318e-07, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                           features\n",
       "0          Gould  [-4.8818765e-07, -9.2718983e-07, -6.4910273e-0...\n",
       "1         Schiff  [-0.008452152, -0.017701644, -0.018430043, -0....\n",
       "2       Horowitz  [-7.096678e-07, -1.4826655e-06, -1.5776604e-06...\n",
       "3        Richter  [-1.6298145e-09, 1.3969839e-09, -1.7462298e-09...\n",
       "4        Crochet  [-2.7939677e-09, -1.3969839e-09, -1.3969839e-0...\n",
       "...          ...                                                ...\n",
       "1195  Pogorelich  [-5.005859e-09, -6.4028427e-09, -2.3283064e-09...\n",
       "1196  Pogorelich  [-9.313226e-10, -1.2805685e-09, -5.820766e-10,...\n",
       "1197       Gould  [1.7369166e-07, 3.091991e-07, 2.5704503e-07, 2...\n",
       "1198      Schiff  [-1.6778418e-10, -1.03521386e-10, -3.929264e-1...\n",
       "1199       Gould  [1.4994293e-07, 2.9057264e-07, 2.477318e-07, 2...\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(initial_features, trainer, extractor, id2label, CONFIDENCE_THRESHOLD=0.5):\n",
    "    encoding = extractor(initial_features, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(trainer.model.device) for k, v in encoding.items()}\n",
    "\n",
    "    outputs = trainer.model(**encoding)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # apply sigmoid + threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    predictions = np.zeros(probs.shape)\n",
    "\n",
    "    predictions[np.where(probs >= CONFIDENCE_THRESHOLD)] = 1\n",
    "    # turn predicted id's into actual label names\n",
    "    predicted_labels = [\n",
    "        id2label[idx] for idx, label in enumerate(predictions) if label == 1.0\n",
    "    ]\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_TO_EVALUATE = 500\n",
    "CONFIDENCE_THRESHOLD = 0.6\n",
    "\n",
    "df_test[\"predicted_label\"] = df_test[\"features\"][0:ROWS_TO_EVALUATE].apply(\n",
    "    inference, args=(trainer, extractor, id2label, CONFIDENCE_THRESHOLD)\n",
    ")\n",
    "\n",
    "df_test[\"correct\"] = df_test.apply(\n",
    "    lambda row: int(str(row[\"label\"]) in str(row[\"predicted_label\"])), axis=1\n",
    ")\n",
    "\n",
    "df_test[\"top_n_preds\"] = df_test[\"features\"][0:ROWS_TO_EVALUATE].apply(\n",
    "    n_most_likely_classes, args=(trainer, extractor, id2label, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>correct</th>\n",
       "      <th>top_n_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[-4.8818765e-07, -9.2718983e-07, -6.4910273e-0...</td>\n",
       "      <td>[Horowitz, Richter, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Tureck': 0.654, 'Richter': 0.639, 'Horowitz'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schiff</td>\n",
       "      <td>[-0.008452152, -0.017701644, -0.018430043, -0....</td>\n",
       "      <td>[Horowitz, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.661, 'Tureck': 0.611, 'Schiff':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Horowitz</td>\n",
       "      <td>[-7.096678e-07, -1.4826655e-06, -1.5776604e-06...</td>\n",
       "      <td>[Horowitz, Ishizaka]</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Horowitz': 0.699, 'Ishizaka': 0.657, 'Richte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Richter</td>\n",
       "      <td>[-1.6298145e-09, 1.3969839e-09, -1.7462298e-09...</td>\n",
       "      <td>[Horowitz]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.753, 'Tureck': 0.57, 'Richter':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crochet</td>\n",
       "      <td>[-2.7939677e-09, -1.3969839e-09, -1.3969839e-0...</td>\n",
       "      <td>[Richter, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Tureck': 0.638, 'Richter': 0.621, 'Horowitz'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Richter</td>\n",
       "      <td>[-8.940697e-08, 2.6077032e-08, -4.7683716e-07,...</td>\n",
       "      <td>[Horowitz]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.674, 'Tureck': 0.6, 'Ishizaka':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[-1.6746654e-10, -7.866863e-11, -1.3521877e-10...</td>\n",
       "      <td>[Horowitz, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Tureck': 0.666, 'Horowitz': 0.652, 'Ishizaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Tureck</td>\n",
       "      <td>[-3.434252e-08, -1.14087015e-08, -3.934838e-08...</td>\n",
       "      <td>[Richter]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Richter': 0.615, 'Moravec': 0.581, 'Ishizaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Gould</td>\n",
       "      <td>[-2.0256266e-07, -3.259629e-07, -2.9057264e-07...</td>\n",
       "      <td>[Horowitz, Ishizaka, Tureck]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.644, 'Tureck': 0.64, 'Ishizaka'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Pogorelich</td>\n",
       "      <td>[9.313226e-10, 1.8626451e-09, 9.313226e-10, 9....</td>\n",
       "      <td>[Horowitz, Richter]</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Horowitz': 0.674, 'Richter': 0.646, 'Schiff'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                           features  \\\n",
       "0         Gould  [-4.8818765e-07, -9.2718983e-07, -6.4910273e-0...   \n",
       "1        Schiff  [-0.008452152, -0.017701644, -0.018430043, -0....   \n",
       "2      Horowitz  [-7.096678e-07, -1.4826655e-06, -1.5776604e-06...   \n",
       "3       Richter  [-1.6298145e-09, 1.3969839e-09, -1.7462298e-09...   \n",
       "4       Crochet  [-2.7939677e-09, -1.3969839e-09, -1.3969839e-0...   \n",
       "..          ...                                                ...   \n",
       "495     Richter  [-8.940697e-08, 2.6077032e-08, -4.7683716e-07,...   \n",
       "496       Gould  [-1.6746654e-10, -7.866863e-11, -1.3521877e-10...   \n",
       "497      Tureck  [-3.434252e-08, -1.14087015e-08, -3.934838e-08...   \n",
       "498       Gould  [-2.0256266e-07, -3.259629e-07, -2.9057264e-07...   \n",
       "499  Pogorelich  [9.313226e-10, 1.8626451e-09, 9.313226e-10, 9....   \n",
       "\n",
       "                  predicted_label  correct  \\\n",
       "0     [Horowitz, Richter, Tureck]        0   \n",
       "1              [Horowitz, Tureck]        0   \n",
       "2            [Horowitz, Ishizaka]        1   \n",
       "3                      [Horowitz]        0   \n",
       "4               [Richter, Tureck]        0   \n",
       "..                            ...      ...   \n",
       "495                    [Horowitz]        0   \n",
       "496            [Horowitz, Tureck]        0   \n",
       "497                     [Richter]        0   \n",
       "498  [Horowitz, Ishizaka, Tureck]        0   \n",
       "499           [Horowitz, Richter]        0   \n",
       "\n",
       "                                           top_n_preds  \n",
       "0    {'Tureck': 0.654, 'Richter': 0.639, 'Horowitz'...  \n",
       "1    {'Horowitz': 0.661, 'Tureck': 0.611, 'Schiff':...  \n",
       "2    {'Horowitz': 0.699, 'Ishizaka': 0.657, 'Richte...  \n",
       "3    {'Horowitz': 0.753, 'Tureck': 0.57, 'Richter':...  \n",
       "4    {'Tureck': 0.638, 'Richter': 0.621, 'Horowitz'...  \n",
       "..                                                 ...  \n",
       "495  {'Horowitz': 0.674, 'Tureck': 0.6, 'Ishizaka':...  \n",
       "496  {'Tureck': 0.666, 'Horowitz': 0.652, 'Ishizaka...  \n",
       "497  {'Richter': 0.615, 'Moravec': 0.581, 'Ishizaka...  \n",
       "498  {'Horowitz': 0.644, 'Tureck': 0.64, 'Ishizaka'...  \n",
       "499  {'Horowitz': 0.674, 'Richter': 0.646, 'Schiff'...  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[0:ROWS_TO_EVALUATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[0:ROWS_TO_EVALUATE].correct.sum() / ROWS_TO_EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
